# -*- coding: utf-8 -*-
import os, requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

BOT = os.getenv("BOT_TOKEN")
CHAT = os.getenv("CHAT_ID")
TG_URL = f"https://api.telegram.org/bot{BOT}/sendMessage"

def send(msg: str):
    requests.post(TG_URL, data={"chat_id": CHAT, "text": msg}, timeout=20)

HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://finance.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8"
}

URLS = {
    "ê¸°ê´€(KOSPI)":   "https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=1000",
    "ê¸°ê´€(KOSDAQ)":  "https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=1000",
    "ì™¸êµ­ì¸(KOSPI)": "https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=2000",
    "ì™¸êµ­ì¸(KOSDAQ)":"https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=2000",
}

def parse_page(url):
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.encoding = "euc-kr"
    soup = BeautifulSoup(resp.text, "html.parser")

    # âœ… ë‚ ì§œ í™•ì¸ (YY.MM.DD í˜•ì‹, ì˜ˆ: 25.08.21)
    today = datetime.now(timezone(timedelta(hours=9))).strftime("%y.%m.%d")
    date_elems = soup.select("div.subtop_sise_graph2 > div")  # ë‘ ê°œ divê°€ ìˆìŒ
    date_texts = [d.get_text(strip=True) for d in date_elems]
    if not any(today in t for t in date_texts):
        raise ValueError(f"ë‚ ì§œ ë¶ˆì¼ì¹˜ (today={today}, page={date_texts})")

    # ì¢…ëª©ëª…/ê¸ˆì•¡ ì¶”ì¶œ
    data = []
    for tr in soup.select("table.type_2 tr"):
        tds = tr.find_all("td")
        if len(tds) < 4:
            continue
        name_tag = tds[0].select_one("p > a")
        amt_tag  = tds[2]
        if not name_tag or not amt_tag:
            continue
        name = name_tag.get_text(strip=True)
        amt_str = amt_tag.get_text(strip=True).replace(",", "")
        if not amt_str.isdigit():
            continue
        amt = int(amt_str)
        data.append((name, amt))
    return data

def main():
    all_data = []
    for key, url in URLS.items():
        try:
            rows = parse_page(url)
            all_data.extend(rows)
        except Exception as e:
            send(f"âŒ {key} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return

    # âœ… ì¢…ëª© ìˆ˜ í™•ì¸
    if len(all_data) != 80:
        send(f"âŒ ì˜¤ë¥˜ë°œìƒ: ì¢…ëª© ìˆ˜ ë¶ˆì¼ì¹˜ (len={len(all_data)})")
        return

    # âœ… ê¸ˆì•¡ ê¸°ì¤€ ì •ë ¬ í›„ ìƒìœ„ 25
    top25 = sorted(all_data, key=lambda x: x[1], reverse=True)[:25]

    today = datetime.now(timezone(timedelta(hours=9))).strftime("%y.%m.%d")
    lines = [f"ğŸ“ˆ {today} ì¥ë§ˆê° ìˆœë§¤ìˆ˜ ìƒìœ„ TOP25", ""]
    for i, (name, amt) in enumerate(top25, 1):
        lines.append(f"{i}. {name} {amt:,}ë°±ë§Œ")

    send("\n".join(lines))

if __name__ == "__main__":
    main()
