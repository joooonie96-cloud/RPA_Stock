# -*- coding: utf-8 -*-
# ë„¤ì´ë²„ ì¦ê¶Œ "ê¸°ê´€/ì™¸êµ­ì¸ ìˆœë§¤ë§¤ ìƒìœ„ (ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥)" í¬ë¡¤ë§ â†’ í…”ë ˆê·¸ë¨ ë°œì†¡
# í•„ìš” íŒ¨í‚¤ì§€: requests, beautifulsoup4
# pip install requests beautifulsoup4

import os, requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

BOT = os.getenv("BOT_TOKEN")
CHAT = os.getenv("CHAT_ID")
if not BOT or not CHAT:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ BOT_TOKEN/CHAT_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

TG_URL = f"https://api.telegram.org/bot{BOT}/sendMessage"

def send(msg: str):
    try:
        r = requests.post(TG_URL, data={"chat_id": CHAT, "text": msg}, timeout=20)
        r.raise_for_status()
    except Exception as e:
        print("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨:", e)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    ),
    "Referer": "https://finance.naver.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

# ë„¤ì´ë²„ ì‹¤ì œ í™”ë©´ ë§¤í•‘ (ë„¤ê°€ í™•ì¸í•œ ë§¤í•‘ ê¸°ì¤€)
URLS = {
    "ê¸°ê´€(KOSPI)":   "https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=1000",
    "ê¸°ê´€(KOSDAQ)":  "https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=1000",
    "ì™¸êµ­ì¸(KOSPI)": "https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=2000",
    "ì™¸êµ­ì¸(KOSDAQ)":"https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=2000",
}

def parse_table(table) -> list:
    """type_2 í…Œì´ë¸” í•˜ë‚˜ì—ì„œ TOP10 í–‰ì„ íŒŒì‹±í•˜ì—¬ ë°˜í™˜."""
    rows = []
    for tr in table.select("tr"):
        tds = tr.find_all("td")
        if len(tds) < 7:
            continue
        name = tds[1].get_text(strip=True)
        amt  = tds[-1].get_text(strip=True)  # ê±°ë˜ê¸ˆì•¡(ë°±ë§Œ)
        if not name or name == "í•©ê³„":
            continue
        rows.append(f"{len(rows)+1}. {name} {amt}ë°±ë§Œ")
        if len(rows) >= 10:
            break
    return rows

def fetch_one(url: str) -> dict:
    """
    ë°˜í™˜ ì˜ˆ:
    {
      "buy": [...],    # ìˆœë§¤ìˆ˜ í‘œ(ìˆìœ¼ë©´)
      "sell": [...],   # ìˆœë§¤ë„ í‘œ(ìˆìœ¼ë©´)
      "debug": "..."   # ë¬¸ì œì‹œ HTML ì•ë¶€ë¶„
    }
    """
    out = {"buy": [], "sell": [], "debug": ""}
    resp = requests.get(url, headers=HEADERS, timeout=20)
    if resp.status_code != 200:
        out["debug"] = f"HTTP ì˜¤ë¥˜ (status {resp.status_code})"
        return out

    resp.encoding = "euc-kr"
    soup = BeautifulSoup(resp.text, "html.parser")
    tables = soup.select("table.type_2")

    if not tables:
        out["debug"] = "í…Œì´ë¸” ì—†ìŒ. ì‘ë‹µ ì•ë¶€ë¶„: " + resp.text[:300].replace("\n", " ")
        return out

    # í‘œê°€ ì—¬ëŸ¿ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í—¤ë”ì— 'ìˆœë§¤ìˆ˜/ìˆœë§¤ë„'ê°€ ìˆëŠ”ì§€ í™•ì¸
    labeled = {"buy": None, "sell": None}
    unknown = []

    for idx, table in enumerate(tables):
        header_text = "".join(th.get_text(strip=True) for th in table.select("th"))
        data = parse_table(table)

        # ë°ì´í„° ì—†ëŠ” í‘œëŠ” ìŠ¤í‚µ
        if not data:
            continue

        if "ìˆœë§¤ìˆ˜" in header_text or "ë§¤ìˆ˜" in header_text:
            labeled["buy"] = data
        elif "ìˆœë§¤ë„" in header_text or "ë§¤ë„" in header_text:
            labeled["sell"] = data
        else:
            unknown.append((idx, data))

    # í—¤ë”ë¡œ ì‹ë³„ë˜ë©´ ê·¸ ê°’ ì‚¬ìš©
    if labeled["buy"]:
        out["buy"] = labeled["buy"]
    if labeled["sell"]:
        out["sell"] = labeled["sell"]

    # í—¤ë”ë¡œ ëª» ì°¾ì•˜ìœ¼ë©´: 1ë²ˆì§¸ í‘œ=ìˆœë§¤ìˆ˜, 2ë²ˆì§¸ í‘œ=ìˆœë§¤ë„ë¡œ ê°€ì •
    if not out["buy"] and unknown:
        out["buy"] = unknown[0][1]
    if not out["sell"] and len(unknown) >= 2:
        out["sell"] = unknown[1][1]

    # ê·¸ë˜ë„ ë¹„ì–´ ìˆìœ¼ë©´ ë””ë²„ê·¸
    if not out["buy"] and not out["sell"]:
        out["debug"] = "ìœ íš¨ ë°ì´í„° ì—†ìŒ. ì‘ë‹µ ì•ë¶€ë¶„: " + resp.text[:300].replace("\n", " ")

    return out

def fetch_from_naver() -> dict:
    """
    ë°˜í™˜ ì˜ˆ:
    {
      "ì™¸êµ­ì¸(KOSPI)": ["1. ...", ...],  # ìˆœë§¤ìˆ˜ TOP10 ìœ„ì£¼ë¡œ ë°˜í™˜
      "ì™¸êµ­ì¸(KOSDAQ)": [...],
      "ê¸°ê´€(KOSPI)":   [...],
      "ê¸°ê´€(KOSDAQ)":  [...],
      "debug": {...}                # ì„¹ì…˜ë³„ ë””ë²„ê·¸ ë©”ì‹œì§€(ìˆì„ ë•Œë§Œ)
    }
    """
    results = {"debug": {}}
    for key, url in URLS.items():
        try:
            one = fetch_one(url)
            if one["buy"]:
                results[key] = one["buy"]
            else:
                # ìˆœë§¤ìˆ˜ í‘œê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì´ìœ ë¥¼ ë‚¨ê¹€
                results[key] = ["ë°ì´í„° ì—†ìŒ"]
                if one["debug"]:
                    results["debug"][key] = one["debug"]
        except Exception as e:
            results[key] = [f"ì—ëŸ¬: {e}"]
    return results

def main():
    kst = timezone(timedelta(hours=9))
    now = datetime.now(kst)
    today = now.strftime("%Y-%m-%d (%a)")

    # ì£¼ë§ ìŠ¤í‚µ (ì›í•˜ë©´ íœ´ì¥ì¼ í…Œì´ë¸”ë„ ì¶”ê°€ ê°€ëŠ¥)
    if now.weekday() >= 5:
        send(f"ğŸ“ˆ {today}\nì˜¤ëŠ˜ì€ ì£¼ë§ì´ë¼ ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë‹¨ 1íšŒ ì‹œë„
    res = fetch_from_naver()

    # ë©”ì‹œì§€ ì¡°ë¦½
    parts = []
    order = ["ì™¸êµ­ì¸(KOSPI)", "ì™¸êµ­ì¸(KOSDAQ)", "ê¸°ê´€(KOSPI)", "ê¸°ê´€(KOSDAQ)"]
    for key in order:
        body = "\n".join(res.get(key, ["ë°ì´í„° ì—†ìŒ"]))
        parts.append(f"ğŸ”¹ {key} ìˆœë§¤ìˆ˜ TOP10\n{body}")

    text = f"ğŸ“ˆ {today} ì¥ë§ˆê° ìˆ˜ê¸‰ ìš”ì•½ (ë„¤ì´ë²„ ì¦ê¶Œ)\n\n" + "\n\n".join(parts)

    # ì„¹ì…˜ë³„ ë””ë²„ê·¸(ìˆì„ ë•Œë§Œ, ë§ë¯¸ì— ì²¨ë¶€)
    dbg = res.get("debug", {})
    if dbg:
        text += "\n\n---\n(ë””ë²„ê·¸)\n" + "\n".join([f"{k}: {v}" for k, v in dbg.items()])

    send(text)

if __name__ == "__main__":
    main()
