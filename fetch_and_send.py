# -*- coding: utf-8 -*-
# ë„¤ì´ë²„ ì¦ê¶Œ "ì™¸êµ­ì¸/ê¸°ê´€ Ã— ì½”ìŠ¤í”¼/ì½”ìŠ¤ë‹¥" ìˆœë§¤ìˆ˜ ìƒìœ„ í¬ë¡¤ë§ â†’ í…”ë ˆê·¸ë¨ ë°œì†¡ (ë‚ ì§œ íƒìƒ‰ ê°•í™”)
# pip install requests beautifulsoup4

import os, re, requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í…”ë ˆê·¸ë¨ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BOT = os.getenv("BOT_TOKEN")
CHAT = os.getenv("CHAT_ID")
if not BOT or not CHAT:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ BOT_TOKEN / CHAT_ID ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
TG_URL = f"https://api.telegram.org/bot{BOT}/sendMessage"

def send(msg: str):
    try:
        r = requests.post(TG_URL, data={"chat_id": CHAT, "text": msg}, timeout=20)
        r.raise_for_status()
    except Exception as e:
        print("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨:", e)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HTTP í—¤ë” / ëŒ€ìƒ URL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    ),
    "Referer": "https://finance.naver.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}

# key: í™”ë©´ ë¼ë²¨, value: (url, investor)
URLS = {
    "ê¸°ê´€(KOSPI)"  : ("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=1000", "ê¸°ê´€"),
    "ê¸°ê´€(KOSDAQ)" : ("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=1000", "ê¸°ê´€"),
    "ì™¸êµ­ì¸(KOSPI)": ("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=2000", "ì™¸êµ­ì¸"),
    "ì™¸êµ­ì¸(KOSDAQ)":("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=2000", "ì™¸êµ­ì¸"),
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATE_REGEX = re.compile(r"\b\d{2}\.\d{2}\.\d{2}\b")  # YY.MM.DD

def find_page_date(soup: BeautifulSoup, raw_html: str) -> str | None:
    """
    ê°€ëŠ¥í•œ ë‚ ì§œ ìœ„ì¹˜ë¥¼ ìˆœì°¨ íƒìƒ‰:
      1) div.sise_guide_date
      2) ìƒë‹¨ ì•ˆë‚´/ê°€ì´ë“œ ì˜ì—­ ì¶”ì • ì„ íƒìë“¤
      3) í˜ì´ì§€ ì „ì²´ í…ìŠ¤íŠ¸ ì •ê·œì‹ íƒìƒ‰ (YY.MM.DD)
    """
    # 1) ê°€ì¥ í™•ì‹¤í•œ ê¸°ì¡´ ì…€ë ‰í„°
    cand = soup.select_one("div.sise_guide_date")
    if cand:
        txt = cand.get_text(strip=True)
        if DATE_REGEX.search(txt):
            return txt

    # 2) ë‹¤ë¥¸ ìƒë‹¨ í›„ë³´ ì…€ë ‰í„° ì‹œë„ (í˜ì´ì§€ ë³€í˜• ëŒ€ì‘)
    candidates = [
        "div.guide_info", "div.guide", "div#content > div h3", "div#content > h3",
        "div.section_sise_top", "div.subtop_sise_graph2", "div.wrap_cont > div"
    ]
    for sel in candidates:
        el = soup.select_one(sel)
        if not el:
            continue
        txt = el.get_text(" ", strip=True)
        m = DATE_REGEX.search(txt)
        if m:
            return m.group(0)

    # 3) ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ìµœì¢… íƒìƒ‰
    m = DATE_REGEX.search(soup.get_text(" ", strip=True))
    if m:
        return m.group(0)

    # ê·¸ë˜ë„ ì‹¤íŒ¨ â†’ raw_html ì•ë¶€ë¶„ì„ ë””ë²„ê·¸ë¡œ í™•ì¸í•˜ê¸° ì¢‹ê²Œ ë°˜í™˜ None
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íŒŒì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def parse_page(url: str):
    """
    - ë‚ ì§œ ê²€ì¦: find_page_date()ë¡œ ì°¾ì€ ê°’ == ì˜¤ëŠ˜(YY.MM.DD)
    - í‘œ íŒŒì‹±: table.type_2 ì˜ tbody > tr ì—ì„œ
        ì¢…ëª©ëª…: ì²« ë²ˆì§¸ td ë‚´ë¶€ì˜ p > a
        ê¸ˆì•¡  : ì„¸ ë²ˆì§¸ td (ë°±ë§Œ ë‹¨ìœ„, ì½¤ë§ˆ ì œê±° í›„ int, ìŒìˆ˜ í—ˆìš©)
    - ë°˜í™˜: [(name:str, amount:int)]
    """
    resp = requests.get(url, headers=HEADERS, timeout=20)
    if resp.status_code != 200:
        raise ValueError(f"HTTP ìƒíƒœì½”ë“œ {resp.status_code}")
    resp.encoding = "euc-kr"
    soup = BeautifulSoup(resp.text, "html.parser")

    # âœ… ë‚ ì§œ í™•ì¸ (YY.MM.DD)
    today = datetime.now(timezone(timedelta(hours=9))).strftime("%y.%m.%d")
    page_date = find_page_date(soup, resp.text)
    if not page_date:
        snippet = resp.text[:200].replace("\n", " ")
        raise ValueError(f"ë‚ ì§œ íƒìƒ‰ ì‹¤íŒ¨. ì‘ë‹µ ì•ë¶€ë¶„: {snippet}")
    if page_date != today:
        raise ValueError(f"ë‚ ì§œ ë¶ˆì¼ì¹˜ (today={today}, page={page_date})")

    # âœ… í‘œ íŒŒì‹± (type_2 í…Œì´ë¸” ì¤‘ ì²« ë²ˆì§¸ í‘œ ê¸°ì¤€)
    table = soup.select_one("table.type_2")
    if not table:
        snippet = resp.text[:200].replace("\n", " ")
        raise ValueError(f"í…Œì´ë¸” ì—†ìŒ(table.type_2). ì‘ë‹µ ì•ë¶€ë¶„: {snippet}")

    data = []
    for tr in table.select("tbody > tr"):
        tds = tr.find_all("td")
        if len(tds) < 3:
            continue
        name_tag = tds[0].select_one("p > a")
        amt_tag  = tds[2]
        if not name_tag or not amt_tag:
            continue
        name = name_tag.get_text(strip=True)
        amt_str = amt_tag.get_text(strip=True).replace(",", "")
        if not amt_str or not amt_str.replace("-", "").isdigit():
            continue
        amt = int(amt_str)
        data.append((name, amt))
    return data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    kst = timezone(timedelta(hours=9))
    today_label = datetime.now(kst).strftime("%y.%m.%d")

    foreign = []  # ì™¸êµ­ì¸ í•©ì‚°
    inst = []     # ê¸°ê´€ í•©ì‚°
    total = 0

    for label, (url, investor) in URLS.items():
        try:
            rows = parse_page(url)
        except Exception as e:
            send(f"âŒ {label} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return
        total += len(rows)
        if investor == "ì™¸êµ­ì¸":
            foreign.extend(rows)
        else:
            inst.extend(rows)

    # 80ì¢…ëª©(ì™¸40+ê¸°40) ê²€ì¦
    if total != 80 or len(foreign) != 40 or len(inst) != 40:
        send(f"âŒ ì˜¤ë¥˜ë°œìƒ: ì¢…ëª© ìˆ˜ ë¶ˆì¼ì¹˜ (ì´={total}, ì™¸êµ­ì¸={len(foreign)}, ê¸°ê´€={len(inst)})")
        return

    # íˆ¬ììë³„ ìƒìœ„ 25
    top25_foreign = sorted(foreign, key=lambda x: x[1], reverse=True)[:25]
    top25_inst    = sorted(inst,    key=lambda x: x[1], reverse=True)[:25]

    lines = [f"ğŸ“ˆ {today_label} ì¥ë§ˆê° ìˆœë§¤ìˆ˜ ìƒìœ„ (ë„¤ì´ë²„ ì¦ê¶Œ)"]
    lines.append("")
    lines.append("ğŸ”¹ ì™¸êµ­ì¸ TOP25")
    for i, (name, amt) in enumerate(top25_foreign, 1):
        lines.append(f"{i}. {name} {amt:,}ë°±ë§Œ")
    lines.append("")
    lines.append("ğŸ”¹ ê¸°ê´€ TOP25")
    for i, (name, amt) in enumerate(top25_inst, 1):
        lines.append(f"{i}. {name} {amt:,}ë°±ë§Œ")

    send("\n".join(lines))

if __name__ == "__main__":
    main()
