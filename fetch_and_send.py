# -*- coding: utf-8 -*-
# ë„¤ì´ë²„ ì¦ê¶Œ 4ê°œ URL ë³‘ë ¬ í¬ë¡¤ë§ â†’ ë‚ ì§œê²€ì¦(YY.MM.DD) â†’ ì™¸êµ­ì¸/ê¸°ê´€ ê°ê° TOP25 í…”ë ˆê·¸ë¨ ë°œì†¡
# í•„ìš”: httpx, beautifulsoup4

import os, re, asyncio, httpx
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# â”€â”€â”€â”€â”€ í…”ë ˆê·¸ë¨ â”€â”€â”€â”€â”€
BOT = os.getenv("BOT_TOKEN")
CHAT = os.getenv("CHAT_ID")
if not BOT or not CHAT:
    raise RuntimeError("í™˜ê²½ë³€ìˆ˜ BOT_TOKEN/CHAT_ID ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
TG_URL = f"https://api.telegram.org/bot{BOT}/sendMessage"

async def send_tg(client: httpx.AsyncClient, text: str):
    await client.post(TG_URL, data={"chat_id": CHAT, "text": text}, timeout=20)

# â”€â”€â”€â”€â”€ ë„¤ì´ë²„ ì„¤ì • â”€â”€â”€â”€â”€
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/115.0.0.0 Safari/537.36"
    ),
    "Referer": "https://finance.naver.com/",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
}
DATE_RX = re.compile(r"\b\d{2}\.\d{2}\.\d{2}\b")  # YY.MM.DD

# key: ë¼ë²¨, val: (url, investor)
URLS = {
    "ê¸°ê´€(KOSPI)"  : ("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=1000", "ê¸°ê´€"),
    "ê¸°ê´€(KOSDAQ)" : ("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=1000", "ê¸°ê´€"),
    "ì™¸êµ­ì¸(KOSPI)": ("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=01&investor_gubun=2000", "ì™¸êµ­ì¸"),
    "ì™¸êµ­ì¸(KOSDAQ)":("https://finance.naver.com/sise/sise_deal_rank.naver?sosok=02&investor_gubun=2000", "ì™¸êµ­ì¸"),
}

def find_date(soup: BeautifulSoup, raw: str) -> str | None:
    # 1) ê°€ì¥ ì•ˆì •ì ì¸ ì§§ì€ ì…€ë ‰í„°
    node = soup.select_one("div.sise_guide_date")
    if node:
        txt = node.get_text(strip=True)
        if DATE_RX.search(txt): return txt
    # 2) fallback: í˜ì´ì§€ ì „ì²´ì—ì„œ YY.MM.DD
    m = DATE_RX.search(soup.get_text(" ", strip=True))
    return m.group(0) if m else None

def parse_rows(html: str, today_fmt: str) -> list[tuple[str, int]]:
    # EUC-KR ì§€ì •
    soup = BeautifulSoup(html, "html.parser")

    # ë‚ ì§œ ê²€ì¦
    page_date = find_date(soup, html)
    if not page_date:
        raise ValueError("ë‚ ì§œ íƒìƒ‰ ì‹¤íŒ¨(div.sise_guide_date ì—†ìŒ)")
    if page_date != today_fmt:
        raise ValueError(f"ë‚ ì§œ ë¶ˆì¼ì¹˜ (today={today_fmt}, page={page_date})")

    # í‘œ íŒŒì‹± (ê°€ì¥ ì¼ë°˜ì ì¸ ì²« ë²ˆì§¸ type_2 í…Œì´ë¸”)
    table = soup.select_one("table.type_2")
    if not table:
        raise ValueError("table.type_2 ì—†ìŒ")

    out = []
    # td ì¸ë±ìŠ¤ë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼: 0=ì¢…ëª©ëª… ì˜ì—­, 2=ê¸ˆì•¡ ì¹¼ëŸ¼(ë°±ë§Œ ë‹¨ìœ„)
    for tr in table.select("tbody > tr"):
        tds = tr.find_all("td")
        if len(tds) < 3:
            continue
        name_tag = tds[0].select_one("p > a")
        amt_tag  = tds[2]
        if not name_tag or not amt_tag:
            continue
        name = name_tag.get_text(strip=True)
        amt_str = amt_tag.get_text(strip=True).replace(",", "")
        if not amt_str or not amt_str.replace("-", "").isdigit():
            continue
        amt = int(amt_str)
        out.append((name, amt))
    return out

async def fetch_one(client: httpx.AsyncClient, url: str) -> str:
    r = await client.get(url, headers=HEADERS, timeout=20)
    if r.status_code != 200:
        raise ValueError(f"HTTP {r.status_code}")
    # ê°•ì œ EUC-KR ë””ì½”ë”©
    r.encoding = "euc-kr"
    return r.text

async def main():
    kst = timezone(timedelta(hours=9))
    today_label = datetime.now(kst).strftime("%y.%m.%d")

    async with httpx.AsyncClient(http2=True) as client:
        # 4ê°œ URL ë™ì‹œ ìš”ì²­
        tasks = {label: fetch_one(client, u) for label, (u, _) in URLS.items()}
        html_map = {}
        try:
            htmls = await asyncio.gather(*tasks.values(), return_exceptions=True)
            for (label, _), html in zip(URLS.items(), htmls):
                if isinstance(html, Exception):
                    raise html
                html_map[label] = html
        except Exception as e:
            await send_tg(client, f"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}")
            return

        # íŒŒì‹± + ë‚ ì§œê²€ì¦ + ì¹´í…Œê³ ë¦¬ ëª¨ìœ¼ê¸°
        foreign, inst = [], []
        total = 0
        for label, (url, who) in URLS.items():
            raw = html_map.get(label, "")
            try:
                rows = parse_rows(raw, today_label)
            except Exception as e:
                snippet = (raw[:180].replace("\n", " ") if raw else "ì‘ë‹µ ì—†ìŒ")
                await send_tg(client, f"âŒ {label} íŒŒì‹± ì‹¤íŒ¨: {e}\nâ€¦ {snippet}")
                return

            total += len(rows)
            if who == "ì™¸êµ­ì¸":
                foreign.extend(rows)
            else:
                inst.extend(rows)

        # 80ê°œ(ì™¸40+ê¸°40) í™•ì¸
        if total != 80 or len(foreign) != 40 or len(inst) != 40:
            await send_tg(client, f"âŒ ì¢…ëª© ìˆ˜ ë¶ˆì¼ì¹˜: ì´={total}, ì™¸êµ­ì¸={len(foreign)}, ê¸°ê´€={len(inst)} (ê¸°ëŒ€: 80/40/40)")
            return

        # ê¸ˆì•¡ ê¸°ì¤€ ìƒìœ„ 25ì”©
        top25_foreign = sorted(foreign, key=lambda x: x[1], reverse=True)[:25]
        top25_inst    = sorted(inst,    key=lambda x: x[1], reverse=True)[:25]

        # ë©”ì‹œì§€ ì¡°ë¦½
        lines = [f"ğŸ“ˆ {today_label} ì¥ë§ˆê° ìˆœë§¤ìˆ˜ ìƒìœ„ (ë„¤ì´ë²„ ì¦ê¶Œ)"]
        lines.append("")
        lines.append("ğŸ”¹ ì™¸êµ­ì¸ TOP25")
        for i, (name, amt) in enumerate(top25_foreign, 1):
            lines.append(f"{i}. {name} {amt:,}ë°±ë§Œ")
        lines.append("")
        lines.append("ğŸ”¹ ê¸°ê´€ TOP25")
        for i, (name, amt) in enumerate(top25_inst, 1):
            lines.append(f"{i}. {name} {amt:,}ë°±ë§Œ")

        await send_tg(client, "\n".join(lines))

if __name__ == "__main__":
    asyncio.run(main())
