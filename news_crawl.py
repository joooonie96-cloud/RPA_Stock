#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, time, html, re, requests, feedparser
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
from readability import Document
from bs4 import BeautifulSoup
from difflib import SequenceMatcher

# ==== í™˜ê²½ë³€ìˆ˜ ====
BOT_TOKEN = os.getenv("BOT_TOKEN")          # í…”ë ˆê·¸ë¨ ë´‡ í† í°
CHAT_ID   = os.getenv("CHAT_ID")            # í…”ë ˆê·¸ë¨ ì±„íŒ… ID
SHEET_ID  = os.getenv("SHEET_ID_NEWS")      # êµ¬ê¸€ ì‹œíŠ¸ ID
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# ==== ì‹œê°„/ìƒìˆ˜ ====
KST = timezone(timedelta(hours=9))
NOW_KST = datetime.now(KST)
YESTERDAY_KST = (NOW_KST - timedelta(days=1)).date()
TODAY_KST = NOW_KST.date()
YESTERDAY_STR = YESTERDAY_KST.strftime("%Y-%m-%d")
TODAY_STR = TODAY_KST.strftime("%Y-%m-%d")

# '1ì¼' ~ '31ì¼' (ê°ê° ê²€ìƒ‰; ì œëª©ì— ì—†ìœ¼ë©´ ë³¸ë¬¸ íŒŒì‹± - ìˆœì°¨)
DAY_TERMS = [f"{d}ì¼" for d in range(1, 32)]

BASE = "https://news.google.com/rss/search"
COMMON_QS = "hl=ko&gl=KR&ceid=KR:ko"

# ìˆ«ì+ì¼ íŒ¨í„´
DATE_TERM_RE = re.compile(r"(?<!\d)(?:[1-9]|[12]\d|3[01])ì¼(?!\d)")

# HTTP
REQ_TIMEOUT = 12
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
}

# ì œëª© ìœ ì‚¬ë„ ì„ê³„
TITLE_SIM_THRESHOLD = 0.90

# ==== Google Sheets ====
def ensure_gspread():
    if not GOOGLE_APPLICATION_CREDENTIALS or not SHEET_ID:
        raise RuntimeError("Google Sheets ì¸ì¦/ID í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    import gspread
    from google.oauth2.service_account import Credentials
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ]
    creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS, scopes=scopes)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(SHEET_ID)
    return sh

def get_or_create_daily_ws(sh, date_str: str):
    """
    ì˜¤ëŠ˜ íƒ­(YYYY-MM-DD)ì„ ë°˜í™˜. ì—†ìœ¼ë©´ ìƒì„±í•˜ê³  í—¤ë”ë¥¼ ê¸°ë¡.
    ìˆìœ¼ë©´ í—¤ë”ê°€ ë¹„ì–´ìˆì„ ë•Œë§Œ í—¤ë”ë¥¼ ë³´ì •.
    """
    import gspread
    try:
        ws = sh.worksheet(date_str)
        # í—¤ë” í™•ì¸/ë³´ì •
        first_row = ws.row_values(1)
        need_header = (len(first_row) < 4) or (first_row[:4] != ["ê¸°ì‚¬ ì œëª©", "URL", "ê²Œì‹œì‹œê°(KST)", "ë§¤ì¹­ í‚¤ì›Œë“œ"])
        if need_header:
            ws.insert_row(["ê¸°ì‚¬ ì œëª©", "URL", "ê²Œì‹œì‹œê°(KST)", "ë§¤ì¹­ í‚¤ì›Œë“œ"], 1)
        created = False
    except gspread.WorksheetNotFound:
        ws = sh.add_worksheet(title=date_str, rows=6000, cols=4)
        ws.append_row(["ê¸°ì‚¬ ì œëª©", "URL", "ê²Œì‹œì‹œê°(KST)", "ë§¤ì¹­ í‚¤ì›Œë“œ"])
        created = True
    return ws, created

def load_existing_index(ws):
    """
    ì˜¤ëŠ˜ íƒ­ì— ì´ë¯¸ ì ì¬ëœ URL/ì œëª©ì„ ë¶ˆëŸ¬ì™€ì„œ set/listë¡œ ë°˜í™˜.
    - URLì€ ì¤‘ë³µ ë°©ì§€ì˜ ì ˆëŒ€ í‚¤
    - ì œëª©ì€ ìœ ì‚¬ë„ 90% ì´ìƒ ì¤‘ë³µ ë°©ì§€ìš©
    """
    # ì „ì²´ ê°’ì—ì„œ ì²« í–‰ì€ í—¤ë”ì´ë¯€ë¡œ ì œì™¸
    all_vals = ws.get_all_values()
    if not all_vals or len(all_vals) == 1:
        return set(), []
    rows = all_vals[1:]  # exclude header
    urls = set()
    titles = []
    for r in rows:
        # ì»¬ëŸ¼ ìˆœì„œ: [ì œëª©, URL, ê²Œì‹œì‹œê°, ë§¤ì¹­ í‚¤ì›Œë“œ]
        if len(r) >= 2:
            url = r[1].strip()
            if url:
                urls.add(url)
        if len(r) >= 1:
            t = r[0].strip()
            if t:
                titles.append(t)
    return urls, titles

def title_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a.strip(), b.strip()).ratio()

def should_skip_by_title_sim(title: str, existing_titles: list) -> bool:
    if not existing_titles:
        return False
    return max(title_similarity(title, t) for t in existing_titles) >= TITLE_SIM_THRESHOLD

def write_sheet_append(sh, items):
    """
    ì˜¤ëŠ˜ íƒ­ì— 'ëˆ„ì  append'.
    - ê°™ì€ URLì€ ìŠ¤í‚µ
    - ê¸°ì¡´ ì œëª©ê³¼ ìœ ì‚¬ë„ 90% ì´ìƒì´ë©´ ìŠ¤í‚µ
    - ìƒˆë¡œ ë“¤ì–´ê°„ ê±´ìˆ˜ì™€ ì˜¤ëŠ˜ íƒ­ URLì„ ë°˜í™˜
    """
    ws, _ = get_or_create_daily_ws(sh, TODAY_STR)
    existing_urls, existing_titles = load_existing_index(ws)

    rows_to_append = []
    added_count = 0

    for it in items:
        title = it["title"].strip()
        url   = it["link"].strip()
        if not url or url in existing_urls:
            continue
        if should_skip_by_title_sim(title, existing_titles):
            continue
        rows_to_append.append([
            title,
            url,
            it["published_kst"].strftime("%Y-%m-%d %H:%M"),
            ", ".join(sorted(it["matched_terms"])) if it.get("matched_terms") else ""
        ])
        # ë¯¸ë¦¬ ì§‘í•©/ë¦¬ìŠ¤íŠ¸ì— ë°˜ì˜í•´ ê°™ì€ ì‹¤í–‰ ì•ˆì—ì„œë„ ì¤‘ë³µ ë°©ì§€
        existing_urls.add(url)
        existing_titles.append(title)
        added_count += 1

    if rows_to_append:
        ws.append_rows(rows_to_append, value_input_option="RAW")

    sheet_url = f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/edit#gid={ws.id}"
    return sheet_url, added_count

# ==== ìˆ˜ì§‘ ====
def fetch_entries_for_term(term: str):
    url = f"{BASE}?q={quote(term)}&{COMMON_QS}"
    feed = feedparser.parse(url)
    return feed.entries or []

def parse_published(entry):
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc)
    return datetime.now(timezone.utc)

def is_within_yesterday_or_today(pub_dt_utc: datetime) -> bool:
    kst = pub_dt_utc.astimezone(KST)
    return kst.date() in {YESTERDAY_KST, TODAY_KST}

def extract_article_text(url: str) -> str:
    """ì œëª©ì— ë‚ ì§œê°€ ì—†ì„ ë•Œë§Œ í˜¸ì¶œ (ìˆœì°¨ íŒŒì‹±)"""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, allow_redirects=True)
        resp.raise_for_status()
        doc = Document(resp.text or "")
        summary_html = doc.summary()
        soup = BeautifulSoup(summary_html, "lxml")
        text = soup.get_text(separator="\n", strip=True)
        text = re.sub(r"\s+\n", "\n", text)
        text = re.sub(r"\n{2,}", "\n\n", text)
        return text[:15000]
    except Exception:
        return ""

# ==== í…”ë ˆê·¸ë¨ ====
def send_tg(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("BOT_TOKEN/CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    r = requests.post(url, data={
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }, timeout=30)
    r.raise_for_status()

def build_done_message(sheet_url: str, added: int, tried: int):
    hdr = "âœ… ë‰´ìŠ¤ ì ì¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
    rng = f"(ë²”ìœ„: ì–´ì œ {YESTERDAY_STR} ~ ì˜¤ëŠ˜ {TODAY_STR} Â· KST)"
    stat = f"ì´ë²ˆ ì‹¤í–‰: ì‹ ê·œ {added}ê±´ / í›„ë³´ {tried}ê±´"
    link = f"ğŸ“Š ì˜¤ëŠ˜ íƒ­: {html.escape(sheet_url)}"
    return [f"{hdr}\n{rng}\n{stat}\n\n{link}"]

# ==== ë©”ì¸ ====
def main():
    # 1) '1ì¼'~'31ì¼' ê°ê° RSS í˜¸ì¶œ â†’ í›„ë³´ ìƒì„± (matched_terms í¬í•¨)
    raw_candidates = []
    for term in DAY_TERMS:
        for e in fetch_entries_for_term(term):
            title = (e.get("title") or "").strip()
            link  = (e.get("link") or "").strip()
            if not title or not link:
                continue
            pub_utc = parse_published(e)
            if not is_within_yesterday_or_today(pub_utc):
                continue
            raw_candidates.append({
                "title": title,
                "link": link,
                "published_kst": pub_utc.astimezone(KST),
                "matched_terms": {term},   # ì´ ì¿¼ë¦¬ì—ì„œ ì¡í˜”ë‹¤
            })

    # 2) ê°™ì€ URLì´ ì—¬ëŸ¬ í‚¤ì›Œë“œë¡œ ì¡íŒ ê²½ìš°: URL ê¸°ì¤€ìœ¼ë¡œ í•©ì¹˜ê³  í‚¤ì›Œë“œ ë³‘í•©
    by_url = {}
    for it in raw_candidates:
        key = it["link"]
        if key not in by_url:
            by_url[key] = it.copy()
        else:
            by_url[key]["matched_terms"].update(it["matched_terms"])
            if it["published_kst"] > by_url[key]["published_kst"]:
                by_url[key]["published_kst"] = it["published_kst"]
                by_url[key]["title"] = it["title"]

    candidates = list(by_url.values())

    # 3) ë‚ ì§œ í‚¤ì›Œë“œ í•„í„°: ì œëª© í†µê³¼ + (ì œëª© ë¯¸í†µê³¼ëŠ” ë³¸ë¬¸ íŒŒì‹±ìœ¼ë¡œ í™•ì¸; ë³¸ë¬¸ì—ì„œ ì°¾ì€ í‚¤ì›Œë“œë„ ê¸°ë¡)
    matched_items = []
    for it in candidates:
        if DATE_TERM_RE.search(it["title"]):
            found = DATE_TERM_RE.findall(it["title"])
            if found:
                it["matched_terms"].update(found)
            matched_items.append(it)
        else:
            body = extract_article_text(it["link"])
            if body and DATE_TERM_RE.search(body):
                found = DATE_TERM_RE.findall(body)
                if found:
                    it["matched_terms"].update(found)
                matched_items.append(it)

    # 4) ìµœì‹ ìˆœ ì •ë ¬
    matched_items.sort(key=lambda x: x["published_kst"], reverse=True)

    # 5) ì˜¤ëŠ˜ íƒ­ì— ëˆ„ì  append (URL ì¤‘ë³µ/ì œëª© ìœ ì‚¬ 90% ì´ìƒ ìŠ¤í‚µ)
    sh = ensure_gspread()
    sheet_url, added_count = write_sheet_append(sh, matched_items)

    # 6) í…”ë ˆê·¸ë¨: ì™„ë£Œ ì•Œë¦¼(ì–´ì œ/ì˜¤ëŠ˜ ë‚ ì§œ ëª…ì‹œ + ì´ë²ˆ ì‹¤í–‰ í†µê³„ + ì˜¤ëŠ˜ íƒ­ ë§í¬)
    for t in build_done_message(sheet_url, added_count, len(matched_items)):
        send_tg(t)

if __name__ == "__main__":
    main()
