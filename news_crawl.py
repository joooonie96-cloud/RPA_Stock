#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, time, html, re, requests, feedparser
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
from readability import Document
from bs4 import BeautifulSoup

# === í™˜ê²½ë³€ìˆ˜ (í•„ìˆ˜) ===
BOT_TOKEN = os.getenv("BOT_TOKEN")   # í…”ë ˆê·¸ë¨ ë´‡ í† í°
CHAT_ID   = os.getenv("CHAT_ID")     # ìˆ˜ì‹  ì±„íŒ… ID(ê°œì¸/ê·¸ë£¹)

# === ìƒìˆ˜ ===
KST = timezone(timedelta(hours=9))
NOW_KST = datetime.now(KST)
YESTERDAY_KST = (NOW_KST - timedelta(days=1)).date()
TODAY_KST = NOW_KST.date()

# '1ì¼' ~ '31ì¼' (ê²€ìƒ‰ì–´)
DAY_TERMS = [f"{d}ì¼" for d in range(1, 32)]

# Google News RSS (í•œêµ­ì–´/í•œêµ­)
BASE = "https://news.google.com/rss/search"
COMMON_QS = "hl=ko&gl=KR&ceid=KR:ko"

# ìˆ«ì+ì¼ íŒ¨í„´(í•œêµ­ì–´ ë‹¨ì–´ ê²½ê³„ ë³´ì™„)
DATE_TERM_RE = re.compile(r"(?<!\d)(?:[1-9]|[12]\d|3[01])ì¼(?!\d)")

# HTTP ê³µí†µ
REQ_TIMEOUT = 15
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
}

def fetch_entries_for_term(term: str):
    """íŠ¹ì • term(ì˜ˆ: '3ì¼')ìœ¼ë¡œ Google News RSS ê²€ìƒ‰ í›„ entry ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    url = f"{BASE}?q={quote(term)}&{COMMON_QS}"
    feed = feedparser.parse(url)
    return feed.entries or []

def is_within_yesterday_or_today(pub_dt_utc: datetime) -> bool:
    """UTC -> KSTë¡œ ë³€í™˜í•˜ì—¬ ì–´ì œ/ì˜¤ëŠ˜ ê¸°ì‚¬ ì—¬ë¶€ íŒë‹¨"""
    kst = pub_dt_utc.astimezone(KST)
    return kst.date() in {YESTERDAY_KST, TODAY_KST}

def parse_published(entry):
    """feedparserì˜ published_parsedë¥¼ datetime(UTC)ë¡œ"""
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc)
    # ì—†ì„ ê²½ìš°, ì§€ê¸ˆ ì‹œê°ìœ¼ë¡œ ë³´ì •(ë“œë¬¼ê²Œ ë°œìƒ)
    return datetime.now(timezone.utc)

def dedupe(items):
    """ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±°"""
    seen = set()
    out = []
    for it in items:
        key = it["link"]
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def extract_article_text(url: str) -> str:
    """
    ë‰´ìŠ¤ í˜ì´ì§€ HTMLì„ ë°›ì•„ main content í…ìŠ¤íŠ¸ ì¶”ì¶œ.
    - readabilityë¡œ ë³¸ë¬¸ ì¶”ì¶œ â†’ bs4ë¡œ í…ìŠ¤íŠ¸í™”
    """
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, allow_redirects=True)
        resp.raise_for_status()
        # ì¼ë¶€ í¬í„¸(íŠ¹íˆ êµ¬ê¸€ë‰´ìŠ¤ ì¤‘ê°„ ë¦¬ë‹¤ì´ë ‰íŠ¸)ì´ HTMLì´ ì•„ë‹Œ ê²½ìš°ê°€ ìˆì–´ ê°€ë“œ
        html_text = resp.text or ""
        doc = Document(html_text)
        summary_html = doc.summary()
        soup = BeautifulSoup(summary_html, "lxml")
        text = soup.get_text(separator="\n", strip=True)
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r"\s+\n", "\n", text)
        text = re.sub(r"\n{2,}", "\n\n", text)
        return text[:15000]  # ì•ˆì „ìƒ ìµœëŒ€ ê¸¸ì´ ì œí•œ
    except Exception:
        return ""

def build_message(items):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€(4096ì ì œí•œ ê³ ë ¤, ê¸¸ë©´ ë¶„í• )"""
    if not items:
        return ["ì–´ì œ/ì˜¤ëŠ˜ ê¸°ì¤€ìœ¼ë¡œ '1ì¼'~'31ì¼' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."]

    lines = []
    for i, it in enumerate(items, 1):
        title = html.escape(it["title"])
        link  = html.escape(it["link"])
        kst_ts = it["published_kst"].strftime("%Y-%m-%d %H:%M")
        lines.append(f"â€¢ {title}\n{link}  <i>({kst_ts} KST)</i>")

    text = "\n\n".join(lines)
    max_len = 4096
    if len(text) <= max_len:
        return [text]

    # ë„ˆë¬´ ê¸¸ë©´ ì ì ˆíˆ ë¶„í• 
    chunks, chunk, size = [], [], 0
    for block in lines:
        block += "\n\n"
        if size + len(block) > max_len:
            chunks.append("".join(chunk))
            chunk, size = [block], len(block)
        else:
            chunk.append(block)
            size += len(block)
    if chunk:
        chunks.append("".join(chunk))
    return chunks

def send_telegram(text: str):
    """í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡"""
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("BOT_TOKEN/CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    resp = requests.post(url, data={
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }, timeout=30)
    resp.raise_for_status()

def main():
    # 1) ê° í‚¤ì›Œë“œë¡œ RSS ìˆ˜ì§‘ (ì œëª© ê¸°ì¤€ 1ì°¨ í•„í„°)
    candidates = []
    for term in DAY_TERMS:
        for e in fetch_entries_for_term(term):
            title = e.get("title", "").strip()
            link  = e.get("link", "").strip()
            if not title or not link:
                continue
            pub_utc = parse_published(e)
            if not is_within_yesterday_or_today(pub_utc):
                continue
            candidates.append({
                "title": title,
                "link": link,
                "published_kst": pub_utc.astimezone(KST)
            })

    # 2) ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    candidates = dedupe(candidates)

    # 3) ì œëª©/ë³¸ë¬¸ì— ë‚ ì§œ íŒ¨í„´ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ ìµœì¢… í•„í„°
    results = []
    for it in candidates:
        title = it["title"]
        if DATE_TERM_RE.search(title):
            results.append(it)
            continue
        # ì œëª©ì— ì—†ìœ¼ë©´ ë³¸ë¬¸ì„ ë‚´ë ¤ë°›ì•„ ê²€ì‚¬
        article_text = extract_article_text(it["link"])
        if article_text and DATE_TERM_RE.search(article_text):
            results.append(it)

    # ìµœì‹ ìˆœ ì •ë ¬
    results.sort(key=lambda x: x["published_kst"], reverse=True)

    # 4) ë©”ì‹œì§€ ë¹Œë“œ & ë°œì†¡
    texts = build_message(results)
    header = f"ğŸ“° ì–´ì œ/ì˜¤ëŠ˜ '1ì¼~31ì¼' í‚¤ì›Œë“œ(ì œëª©/ë³¸ë¬¸) í¬í•¨ ê¸°ì‚¬ ({NOW_KST.strftime('%Y-%m-%d %H:%M')} ê¸°ì¤€)"
    if texts:
        first = f"{header}\n\n{texts[0]}"
        send_telegram(first)
        for t in texts[1:]:
            send_telegram(t)

if __name__ == "__main__":
    main()
