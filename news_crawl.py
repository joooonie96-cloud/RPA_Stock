#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, time, html, re, requests, feedparser
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
from readability import Document
from bs4 import BeautifulSoup
from difflib import SequenceMatcher

# ==== í™˜ê²½ë³€ìˆ˜ ====
BOT_TOKEN = os.getenv("BOT_TOKEN")         # í…”ë ˆê·¸ë¨ ë´‡ í† í°
CHAT_ID   = os.getenv("CHAT_ID")           # í…”ë ˆê·¸ë¨ ì±„íŒ… ID
SHEET_ID  = os.getenv("SHEET_ID_NEWS")     # êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# ==== ì‹œê°„/ìƒìˆ˜ ====
KST = timezone(timedelta(hours=9))
NOW_KST = datetime.now(KST)
YESTERDAY_KST = (NOW_KST - timedelta(days=1)).date()
TODAY_KST = NOW_KST.date()

DAY_TERMS = [f"{d}ì¼" for d in range(1, 32)]  # '1ì¼' ~ '31ì¼'

BASE = "https://news.google.com/rss/search"
COMMON_QS = "hl=ko&gl=KR&ceid=KR:ko"

# ìˆ«ì+ì¼ íŒ¨í„´(ì •í™•ë„ ë†’ì„: ìˆ«ì 1~31 + 'ì¼' ë‹¨ì–´ ê²½ê³„)
DATE_TERM_RE = re.compile(r"(?<!\d)(?:[1-9]|[12]\d|3[01])ì¼(?!\d)")

REQ_TIMEOUT = 15
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
}

# ==== Sheets ====
def ensure_gspread():
    if not GOOGLE_APPLICATION_CREDENTIALS or not SHEET_ID:
        raise RuntimeError("Google Sheets ì¸ì¦/ID í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    import gspread
    from google.oauth2.service_account import Credentials
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS, scopes=scopes)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(SHEET_ID)
    return sh

def get_or_create_daily_ws(sh, date_str: str):
    import gspread
    try:
        ws = sh.worksheet(date_str)
    except gspread.WorksheetNotFound:
        ws = sh.add_worksheet(title=date_str, rows=2000, cols=3)
    return ws

def write_sheet_all(sh, items):
    """
    ì „ì²´ ê¸°ì‚¬(ì œëª©/URL/ê²Œì‹œì‹œê°KST)ë¥¼ ë‚ ì§œë³„ íƒ­ì— ê¸°ë¡ í›„ ë§í¬ ë°˜í™˜.
    ì—¬ê¸°ì—ì„œ URL ì¤‘ë³µ & ì œëª© ìœ ì‚¬ë„(â‰¥0.70) ì¤‘ë³µì„ ì œê±°í•œë‹¤.
    """
    # 1) URL / ì œëª© ìœ ì‚¬ë„ ì¤‘ë³µ ì œê±°
    items = dedupe_for_sheet(items, title_similarity_threshold=0.70)

    date_tab = TODAY_KST.strftime("%Y-%m-%d")
    ws = get_or_create_daily_ws(sh, date_tab)

    # ì´ˆê¸°í™” â†’ í—¤ë”
    ws.clear()
    ws.append_row(["ê¸°ì‚¬ ì œëª©", "URL", "ê²Œì‹œì‹œê°(KST)"])

    if items:
        rows = [[it["title"], it["link"], it["published_kst"].strftime("%Y-%m-%d %H:%M")] for it in items]
        ws.append_rows(rows, value_input_option="RAW")

    sheet_url = f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/edit#gid={ws.id}"
    return sheet_url, len(items)

# ==== ìˆ˜ì§‘/í•„í„° ====
def fetch_entries_for_term(term: str):
    url = f"{BASE}?q={quote(term)}&{COMMON_QS}"
    feed = feedparser.parse(url)
    return feed.entries or []

def parse_published(entry):
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc)
    return datetime.now(timezone.utc)

def is_within_yesterday_or_today(pub_dt_utc: datetime) -> bool:
    kst = pub_dt_utc.astimezone(KST)
    return kst.date() in {YESTERDAY_KST, TODAY_KST}

def dedupe(items):
    """ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ìˆ˜ì§‘ ë‹¨ê³„)"""
    seen = set()
    out = []
    for it in items:
        key = it["link"]
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def title_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a.strip(), b.strip()).ratio()

def dedupe_for_sheet(items, title_similarity_threshold=0.70):
    """
    ì‹œíŠ¸ ì ì¬ ì „ ì¤‘ë³µ ì œê±°:
      - URL ì™„ì „ ë™ì¼: ì œì™¸
      - ì œëª© ìœ ì‚¬ë„ â‰¥ threshold: ì œì™¸ (ì´ë¯¸ ì„ íƒëœ ê²ƒê³¼ ë¹„êµ)
    """
    seen_urls = set()
    kept = []
    kept_titles = []

    for it in items:
        url = it["link"]
        title = it["title"]
        if url in seen_urls:
            continue
        # ì œëª© ìœ ì‚¬ë„ ë¹„êµ (ì´ë¯¸ ì±„íƒëœ ê²ƒë“¤ê³¼ë§Œ)
        if kept_titles:
            sim = max(title_similarity(title, t) for t in kept_titles)
            if sim >= title_similarity_threshold:
                continue
        kept.append(it)
        kept_titles.append(title)
        seen_urls.add(url)
    return kept

def extract_article_text(url: str) -> str:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, allow_redirects=True)
        resp.raise_for_status()
        doc = Document(resp.text or "")
        summary_html = doc.summary()
        soup = BeautifulSoup(summary_html, "lxml")
        text = soup.get_text(separator="\n", strip=True)
        text = re.sub(r"\s+\n", "\n", text)
        text = re.sub(r"\n{2,}", "\n\n", text)
        return text[:15000]
    except Exception:
        return ""

# ==== â€œì£¼ê°€ì— ì˜í–¥ ì¤„ ë§Œí•œâ€ ê¸°ì‚¬ ì„ ë³„ ====
# ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ìŠ¤ì½”ì–´ëŸ¬ (ì œëª©/ë³¸ë¬¸ í‚¤ì›Œë“œ ë§¤ì¹­)
MAJOR_COMPANY_EVENTS = [
    r"ì‹¤ì |ì˜ì—…ì´ìµ|ìˆœì´ìµ|ê°€ì´ë˜ìŠ¤|ëª©í‘œê°€|ìƒí–¥|í•˜í–¥|ì»¨ì„¼ì„œìŠ¤",
    r"ì¸ìˆ˜|í•©ë³‘|M&A|ë§¤ê°|ì§€ë¶„ ì·¨ë“|ì§€ë¶„ ë§¤ê°|ì „ëµì  ì œíœ´|JV",
    r"ëŒ€ê·œëª¨\s*ìˆ˜ì£¼|ìˆ˜ì£¼ ê³µì‹œ|ê³µê¸‰ ê³„ì•½|ì¥ê¸° ê³„ì•½|ë‚©í’ˆ",
    r"ë¦¬ì½œ|í’ˆì§ˆ ë¬¸ì œ|ì‚¬ê³ |í™”ì¬|ê³µì¥|ë¼ì¸ ì¤‘ë‹¨|íŒŒì—…",
    r"CEO|ëŒ€í‘œì´ì‚¬|ì‚¬ì„|í•´ì„|ì„ ì„|íš¡ë ¹|ë°°ì„|ìˆ˜ì‚¬|ì••ìˆ˜ìˆ˜ìƒ‰",
    r"ì¦ì|ìœ ìƒì¦ì|ê°ì|CB|BW|ì „í™˜ì‚¬ì±„|ë°°ë‹¹|ìì‚¬ì£¼|ì‹ ê·œ ìƒì¥|ìƒì¥íì§€|ê´€ë¦¬ì¢…ëª©",
    r"FDA|í’ˆëª©í—ˆê°€|í—ˆê°€ ì·¨ì†Œ|ì„ìƒ\s*(ì„±ê³µ|ì‹¤íŒ¨)|ê¸´ê¸‰ì‚¬ìš©ìŠ¹ì¸|ì‹ì•½ì²˜|EMA",
    r"ê³µì •ìœ„|ê³¼ì§•ê¸ˆ|ì œì¬|ë‹´í•©|ì¡°ì‚¬ ì°©ìˆ˜|ê²€ì°°",
]

INDUSTRY_WIDE = [
    r"ì—…í™©|ì‚¬ì´í´|ìˆ˜ìš” ë‘”í™”|ìˆ˜ìš” íšŒë³µ|ê°€ê²© ì¸ìƒ|ê°€ê²© ì¸í•˜|ê°ì‚°|ì¦ì‚°",
    r"ë©”ëª¨ë¦¬|DRAM|NAND|ë°˜ë„ì²´ ì¥ë¹„|ë¦¬íŠ¬|ë‹ˆì¼ˆ|ì½”ë°œíŠ¸|ì›ìì¬",
    r"ë³´ì¡°ê¸ˆ|ê·œì œ|ì™„í™”|ì˜ë¬´í™”|ì¹œí™˜ê²½|RE100|íƒ„ì†Œ|ìˆ˜ì¶œì… ê·œì œ",
]

GLOBAL_MACRO = [
    r"ì—°ì¤€|Fed|ê¸ˆë¦¬\s*(ì¸ìƒ|ì¸í•˜|ë™ê²°)|FOMC|ECB|BOJ|ì¤‘êµ­\s*ë¶€ì–‘|í™˜ìœ¨|ë‹¬ëŸ¬|ì—”í™”|ìœ„ì•ˆ",
    r"ìœ ê°€|WTI|ë¸Œë ŒíŠ¸|OPEC|ê°ì‚°",
    r"ì „ìŸ|ë¬´ë ¥|ë¶„ìŸ|ìš°í¬ë¼ì´ë‚˜|ì¤‘ë™|ëŒ€ë§Œ|ì œì¬|ìˆ˜ì¶œí†µì œ|ê´€ì„¸",
]

POLITICAL = [
    r"ëŒ€í†µë ¹|ì´ì¬ëª…|íŠ¸ëŸ¼í”„|ì •ìƒíšŒë‹´|í–‰ì •ëª…ë ¹|ëŒ€ì±…|íŠ¹ë³„ë²•|ì¶”ê²½|ì˜ˆì‚°|ì •ì±… ë°œí‘œ",
]

# ê°€ì¤‘ì¹˜
WEIGHTS = {
    "MAJOR_COMPANY_EVENTS": 4,
    "INDUSTRY_WIDE": 2,
    "GLOBAL_MACRO": 3,
    "POLITICAL": 3,
    # ë³´ì¡° ì‹ í˜¸
    "DATE_CONTEXT": 1,   # 'ì¼ë¶€í„°/ê¹Œì§€/ì/ì‹œí–‰/ë§ˆê°/ê³µê³ /ë°œí‘œ/ê°œìµœ/ì ‘ìˆ˜' ë“±
    "TIME_CONTEXT": 1,   # 'ì˜¤ì „|ì˜¤í›„|ì‹œ|ë¶„' ë“±
}

DATE_CONTEXT = r"ë¶€í„°|ê¹Œì§€|ì|ì‹œí–‰|ë§ˆê°|ê³µê³ |ë°œí‘œ|ê°œìµœ|ì ‘ìˆ˜|ì‹œí•œ|íš¨ë ¥|íš¨ê³¼"
TIME_CONTEXT = r"ì˜¤ì „|ì˜¤í›„|\d{1,2}\s*ì‹œ|\d{1,2}\s*ë¶„"

def _score_with_patterns(text: str, patterns, weight: int):
    score = 0
    for pat in patterns:
        if re.search(pat, text, re.IGNORECASE):
            score += weight
    return score

def market_moving_score(title: str, body: str) -> int:
    """
    ê¸°ì‚¬ ì œëª©/ë³¸ë¬¸ ê¸°ë°˜ ìŠ¤ì½”ì–´. ë†’ì„ìˆ˜ë¡ 'ì£¼ê°€ì— ì˜í–¥' ê°€ëŠ¥ì„±ì´ í¼.
    """
    t = title or ""
    b = body or ""
    full = (t + "\n" + b)

    score = 0
    score += _score_with_patterns(full, MAJOR_COMPANY_EVENTS, WEIGHTS["MAJOR_COMPANY_EVENTS"])
    score += _score_with_patterns(full, INDUSTRY_WIDE, WEIGHTS["INDUSTRY_WIDE"])
    score += _score_with_patterns(full, GLOBAL_MACRO, WEIGHTS["GLOBAL_MACRO"])
    score += _score_with_patterns(full, POLITICAL, WEIGHTS["POLITICAL"])

    # ë³´ì¡° ì‹ í˜¸
    if re.search(DATE_CONTEXT, full):
        score += WEIGHTS["DATE_CONTEXT"]
    if re.search(TIME_CONTEXT, full):
        score += WEIGHTS["TIME_CONTEXT"]

    return score

def filter_market_moving(items):
    """
    ì‹œì¥ì˜í–¥ ê¸°ì‚¬ë§Œ ì„ ë³„: ì œëª©/ë³¸ë¬¸ìœ¼ë¡œ ìŠ¤ì½”ì–´ë§í•´ ì„ê³„ì¹˜ ì´ìƒë§Œ ì±„íƒ.
    ì„ê³„ì¹˜ëŠ” ê²½í—˜ì¹˜ë¡œ 4 ì´ìƒë¶€í„° í†µê³¼(íšŒì‚¬ ì´ë²¤íŠ¸ 1ê°œë§Œ ìˆì–´ë„ í†µê³¼ ê°€ëŠ¥).
    """
    kept = []
    for it in items:
        body = ""
        # ì œëª©ì— ê°•í•œ ì‹ í˜¸ê°€ ì—†ìœ¼ë©´ ë³¸ë¬¸ ì¶”ì¶œí•´ì„œ ì¬í‰ê°€(ë¹„ìš© ì ˆì•½ìš©)
        if not re.search("|".join([*MAJOR_COMPANY_EVENTS, *GLOBAL_MACRO, *POLITICAL]), it["title"], re.IGNORECASE):
            body = extract_article_text(it["link"])
        score = market_moving_score(it["title"], body)
        if score >= 4:
            it["mm_score"] = score
            kept.append(it)
    return kept

# ==== í…”ë ˆê·¸ë¨ ====
def send_tg(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("BOT_TOKEN/CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    r = requests.post(url, data={
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }, timeout=30)
    r.raise_for_status()

def build_top5_message(items, sheet_url: str, sheet_count: int):
    """
    ì‹œì¥ì˜í–¥ ê¸°ì‚¬ ì¤‘ TOP 5ë§Œ ì „ì†¡. ë‚˜ë¨¸ì§€ëŠ” ì‹œíŠ¸ ë§í¬ ì•ˆë‚´.
    """
    total = len(items)
    top = items[:5]
    header = f"ğŸ“ˆ ì‹œì¥ì˜í–¥ ê°€ëŠ¥ì„± ë†’ì€ ê¸°ì‚¬ (ì–´ì œ/ì˜¤ëŠ˜)\nì„ ë³„ {total}ê±´ ì¤‘ TOP 5 ì•„ë˜ â¬‡ï¸\n"

    if not top:
        text = f"{header}\n(í•´ë‹¹ ê¸°ì‚¬ ì—†ìŒ)\n\nğŸ“Š ì „ì²´ ëª©ë¡({sheet_count}ê±´): {html.escape(sheet_url)}"
        return [text]

    lines = []
    for it in top:
        title = it['title'].strip()
        if len(title) > 150:
            title = title[:147] + "â€¦"
        link = it["link"]
        ts = it["published_kst"].strftime("%Y-%m-%d %H:%M")
        score = it.get("mm_score", 0)
        lines.append(f"â€¢ {html.escape(title)}\n{html.escape(link)}  <i>({ts} KST Â· score {score})</i>")

    body = "\n\n".join(lines)
    footer = f"\n\nğŸ“Š ì „ì²´ ëª©ë¡({sheet_count}ê±´): {html.escape(sheet_url)}"
    text = header + "\n" + body + footer

    # 4096ì ë¶„í• 
    if len(text) <= 4096:
        return [text]
    chunks, cur, size = [], [header], len(header)
    for block in lines + [footer]:
        block = "\n\n" + block
        if size + len(block) > 4096:
            chunks.append("".join(cur))
            cur, size = [block], len(block)
        else:
            cur.append(block); size += len(block)
    if cur: chunks.append("".join(cur))
    return chunks

# ==== ë©”ì¸ ====
def main():
    # 1) ìˆ˜ì§‘
    candidates = []
    for term in DAY_TERMS:
        for e in fetch_entries_for_term(term):
            title = e.get("title", "").strip()
            link  = e.get("link", "").strip()
            if not title or not link:
                continue
            pub_utc = parse_published(e)
            if not is_within_yesterday_or_today(pub_utc):
                continue
            candidates.append({
                "title": title,
                "link": link,
                "published_kst": pub_utc.astimezone(KST)
            })

    # 2) ì´ˆê¸° ì¤‘ë³µ ì œê±°
    candidates = dedupe(candidates)

    # 3) ë‚ ì§œ íŒ¨í„´(ì œëª©/ë³¸ë¬¸) í•„í„°
    date_filtered = []
    for it in candidates:
        if DATE_TERM_RE.search(it["title"]):
            date_filtered.append(it)
        else:
            body = extract_article_text(it["link"])
            if body and DATE_TERM_RE.search(body):
                date_filtered.append(it)

    # 4) ìµœì‹ ìˆœ ì •ë ¬
    date_filtered.sort(key=lambda x: x["published_kst"], reverse=True)

    # 5) ì‹œì¥ì˜í–¥ ì„ ë³„ + ì ìˆ˜ ë¶€ì—¬, ì ìˆ˜ DESC â†’ ìµœì‹ ìˆœ tie-break
    market_items = filter_market_moving(date_filtered)
    market_items.sort(key=lambda x: (x.get("mm_score", 0), x["published_kst"]), reverse=True)

    # 6) ì „ì²´(ì¤‘ë³µì œê±° ë²„ì „) â†’ êµ¬ê¸€ ì‹œíŠ¸ ì ì¬ (URL ë™ì¼ Â· ì œëª© ìœ ì‚¬ë„ â‰¥0.70 ì œê±°)
    sh = ensure_gspread()
    sheet_url, sheet_count = write_sheet_all(sh, date_filtered)

    # 7) í…”ë ˆê·¸ë¨: ì‹œì¥ì˜í–¥ TOP 5ë§Œ ë°œì†¡
    texts = build_top5_message(market_items, sheet_url, sheet_count)
    for t in texts:
        send_tg(t)

if __name__ == "__main__":
    main()
