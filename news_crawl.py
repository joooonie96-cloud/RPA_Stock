#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, time, html, re, requests, feedparser
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
from readability import Document
from bs4 import BeautifulSoup

# ==== í™˜ê²½ë³€ìˆ˜ ====
BOT_TOKEN = os.getenv("BOT_TOKEN")         # í…”ë ˆê·¸ë¨ ë´‡ í† í°
CHAT_ID   = os.getenv("CHAT_ID")           # í…”ë ˆê·¸ë¨ ì±„íŒ… ID
SHEET_ID  = os.getenv("SHEET_ID_NEWS")     # êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ID
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# ==== ì‹œê°„/ìƒìˆ˜ ====
KST = timezone(timedelta(hours=9))
NOW_KST = datetime.now(KST)
YESTERDAY_KST = (NOW_KST - timedelta(days=1)).date()
TODAY_KST = NOW_KST.date()

DAY_TERMS = [f"{d}ì¼" for d in range(1, 32)]  # '1ì¼' ~ '31ì¼'

BASE = "https://news.google.com/rss/search"
COMMON_QS = "hl=ko&gl=KR&ceid=KR:ko"

# ìˆ«ì+ì¼ íŒ¨í„´(ì •í™•ë„ ë†’ì„: ìˆ«ì 1~31 + 'ì¼' ë‹¨ì–´ ê²½ê³„)
DATE_TERM_RE = re.compile(r"(?<!\d)(?:[1-9]|[12]\d|3[01])ì¼(?!\d)")

REQ_TIMEOUT = 15
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
}

# ==== Google Sheets ====
def ensure_gspread():
    if not GOOGLE_APPLICATION_CREDENTIALS or not SHEET_ID:
        raise RuntimeError("Google Sheets ì¸ì¦/ID í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    import gspread
    from google.oauth2.service_account import Credentials
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS, scopes=scopes)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(SHEET_ID)
    return sh

def get_or_create_daily_ws(sh, date_str: str):
    """ë‚ ì§œ íƒ­(YYYY-MM-DD) ì—†ìœ¼ë©´ ìƒì„±, ìˆìœ¼ë©´ ë°˜í™˜"""
    try:
        ws = sh.worksheet(date_str)
    except Exception:
        ws = sh.add_worksheet(title=date_str, rows=1000, cols=3)
    return ws

def write_sheet_all(sh, items):
    """ì „ì²´ ê¸°ì‚¬(ì œëª©/URL/ê²Œì‹œì‹œê°KST)ë¥¼ ë‚ ì§œë³„ íƒ­ì— ê¸°ë¡ í›„ ë§í¬ ë°˜í™˜"""
    date_tab = TODAY_KST.strftime("%Y-%m-%d")
    ws = get_or_create_daily_ws(sh, date_tab)
    # ì´ˆê¸°í™” í›„ í—¤ë” + ë°ì´í„°
    ws.clear()
    ws.append_row(["ê¸°ì‚¬ ì œëª©", "URL", "ê²Œì‹œì‹œê°(KST)"])
    rows = []
    for it in items:
        rows.append([it["title"], it["link"], it["published_kst"].strftime("%Y-%m-%d %H:%M")])
    if rows:
        # batch append: ì„±ëŠ¥ ì¢‹ê³  ì•ˆì •ì 
        ws.append_rows(rows, value_input_option="RAW")
    # ì‹œíŠ¸ ë§í¬(gid í¬í•¨)
    sheet_url = f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/edit#gid={ws.id}"
    return sheet_url

# ==== ìˆ˜ì§‘/í•„í„° ====
def fetch_entries_for_term(term: str):
    url = f"{BASE}?q={quote(term)}&{COMMON_QS}"
    feed = feedparser.parse(url)
    return feed.entries or []

def parse_published(entry):
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc)
    return datetime.now(timezone.utc)

def is_within_yesterday_or_today(pub_dt_utc: datetime) -> bool:
    kst = pub_dt_utc.astimezone(KST)
    return kst.date() in {YESTERDAY_KST, TODAY_KST}

def dedupe(items):
    seen = set()
    out = []
    for it in items:
        key = it["link"]
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def extract_article_text(url: str) -> str:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, allow_redirects=True)
        resp.raise_for_status()
        doc = Document(resp.text or "")
        summary_html = doc.summary()
        soup = BeautifulSoup(summary_html, "lxml")
        text = soup.get_text(separator="\n", strip=True)
        text = re.sub(r"\s+\n", "\n", text)
        text = re.sub(r"\n{2,}", "\n\n", text)
        return text[:15000]
    except Exception:
        return ""

# ==== í…”ë ˆê·¸ë¨ ====
def send_tg(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("BOT_TOKEN/CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    r = requests.post(url, data={
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }, timeout=30)
    r.raise_for_status()

def build_top10_message(items, sheet_url: str):
    total = len(items)
    top = items[:10]
    if not top:
        header = f"ğŸ“° ì–´ì œ/ì˜¤ëŠ˜ '1ì¼~31ì¼' í‚¤ì›Œë“œ(ì œëª©/ë³¸ë¬¸) í¬í•¨ ê¸°ì‚¬: 0ê±´\n"
        header += f"ğŸ“Š ì „ì²´ ëª©ë¡: {html.escape(sheet_url)}"
        return [header]

    # 1í–‰ ìš”ì•½ + TOP10
    header = f"ğŸ“° '1ì¼~31ì¼' í¬í•¨ ê¸°ì‚¬ ìš”ì•½ (ì–´ì œ/ì˜¤ëŠ˜ ê¸°ì¤€)\nì´ {total}ê±´ Â· TOP 10 ì•„ë˜ â¬‡ï¸\n"
    lines = []
    for it in top:
        title = it['title'].strip()
        # í…”ë ˆê·¸ë¨ ê¸¸ì´ ìœ„í—˜ ëŒ€ë¹„: ì œëª© ë„ˆë¬´ ê¸¸ë©´ ì»·
        if len(title) > 150:
            title = title[:147] + "â€¦"
        title_esc = html.escape(title)
        link_esc  = html.escape(it["link"])
        ts = it["published_kst"].strftime("%Y-%m-%d %H:%M")
        lines.append(f"â€¢ {title_esc}\n{link_esc}  <i>({ts} KST)</i>")

    body = "\n\n".join(lines)
    footer = f"\n\nğŸ“Š ì „ì²´ ëª©ë¡(ì „ì²´ {total}ê±´): {html.escape(sheet_url)}"
    text = header + "\n" + body + footer

    # í…”ë ˆê·¸ë¨ 4096ì ì œí•œ ê³ ë ¤í•´ ë¶„í• 
    max_len = 4096
    if len(text) <= max_len:
        return [text]

    # ê¸¸ë©´ í—¤ë” + í•­ëª© ë¶„í• 
    chunks, cur, size = [], [header], len(header)
    for block in lines + [footer]:
        block = ("\n\n" + block)
        if size + len(block) > max_len:
            chunks.append("".join(cur))
            cur, size = [block], len(block)
        else:
            cur.append(block)
            size += len(block)
    if cur:
        chunks.append("".join(cur))
    return chunks

# ==== ë©”ì¸ ====
def main():
    # 1) ìˆ˜ì§‘
    candidates = []
    for term in DAY_TERMS:
        for e in fetch_entries_for_term(term):
            title = e.get("title", "").strip()
            link  = e.get("link", "").strip()
            if not title or not link:
                continue
            pub_utc = parse_published(e)
            if not is_within_yesterday_or_today(pub_utc):
                continue
            candidates.append({
                "title": title,
                "link": link,
                "published_kst": pub_utc.astimezone(KST)
            })

    # 2) ì¤‘ë³µ ì œê±°
    candidates = dedupe(candidates)

    # 3) ì œëª©/ë³¸ë¬¸ì— ë‚ ì§œ íŒ¨í„´ ìµœì¢… ê²€ì‚¬
    results = []
    for it in candidates:
        if DATE_TERM_RE.search(it["title"]):
            results.append(it)
            continue
        text = extract_article_text(it["link"])
        if text and DATE_TERM_RE.search(text):
            results.append(it)

    # 4) ìµœì‹ ìˆœ ì •ë ¬
    results.sort(key=lambda x: x["published_kst"], reverse=True)

    # 5) ì „ì²´ â†’ êµ¬ê¸€ ì‹œíŠ¸ ì ì¬
    sh = ensure_gspread()
    sheet_url = write_sheet_all(sh, results)

    # 6) TOP10ë§Œ í…”ë ˆê·¸ë¨
    texts = build_top10_message(results, sheet_url)
    for t in texts:
        send_tg(t)

if __name__ == "__main__":
    main()
