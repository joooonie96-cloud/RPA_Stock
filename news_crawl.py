#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, time, html, re, requests, feedparser
from datetime import datetime, timedelta, timezone
from urllib.parse import quote
from readability import Document
from bs4 import BeautifulSoup
from difflib import SequenceMatcher
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==== í™˜ê²½ë³€ìˆ˜ ====
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID   = os.getenv("CHAT_ID")
SHEET_ID  = os.getenv("SHEET_ID_NEWS")
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

# ==== ì‹œê°„/ìƒìˆ˜ ====
KST = timezone(timedelta(hours=9))
NOW_KST = datetime.now(KST)
YESTERDAY_KST = (NOW_KST - timedelta(days=1)).date()
TODAY_KST = NOW_KST.date()

# '1ì¼' ~ '31ì¼' â†’ OR ê²€ìƒ‰(í•œ ë²ˆì— ìš”ì²­)
DAY_TERMS = [f"{d}ì¼" for d in range(1, 32)]
OR_QUERY = " OR ".join(DAY_TERMS)

BASE = "https://news.google.com/rss/search"
COMMON_QS = "hl=ko&gl=KR&ceid=KR:ko"

# ìˆ«ì+ì¼ íŒ¨í„´(ì •í™•ë„ ë†’ì„)
DATE_TERM_RE = re.compile(r"(?<!\d)(?:[1-9]|[12]\d|3[01])ì¼(?!\d)")

# HTTP
REQ_TIMEOUT = 12
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
}

# ë³‘ë ¬ íŒŒì‹± ì œì–´
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "8"))

# ==== Google Sheets ====
def ensure_gspread():
    if not GOOGLE_APPLICATION_CREDENTIALS or not SHEET_ID:
        raise RuntimeError("Google Sheets ì¸ì¦/ID í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    import gspread
    from google.oauth2.service_account import Credentials
    scopes = [
        "https://www.googleapis.com/auth/spreadsheets",
        "https://www.googleapis.com/auth/drive",
    ]
    creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS, scopes=scopes)
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(SHEET_ID)
    return sh

def get_or_create_daily_ws(sh, date_str: str):
    import gspread
    try:
        ws = sh.worksheet(date_str)
    except gspread.WorksheetNotFound:
        ws = sh.add_worksheet(title=date_str, rows=2000, cols=3)
    return ws

def write_sheet_all(sh, items):
    """
    ì „ì²´ ê¸°ì‚¬(ì œëª©/URL/ê²Œì‹œì‹œê°KST)ë¥¼ ë‚ ì§œë³„ íƒ­ì— ê¸°ë¡.
    - URL ë™ì¼ ì œê±°
    - ì œëª© ìœ ì‚¬ë„ â‰¥0.70 ì œê±°
    """
    items = dedupe_for_sheet(items, title_similarity_threshold=0.70)

    date_tab = TODAY_KST.strftime("%Y-%m-%d")
    ws = get_or_create_daily_ws(sh, date_tab)
    ws.clear()
    ws.append_row(["ê¸°ì‚¬ ì œëª©", "URL", "ê²Œì‹œì‹œê°(KST)"])

    if items:
        rows = [[it["title"], it["link"], it["published_kst"].strftime("%Y-%m-%d %H:%M")] for it in items]
        ws.append_rows(rows, value_input_option="RAW")

    sheet_url = f"https://docs.google.com/spreadsheets/d/{SHEET_ID}/edit#gid={ws.id}"
    return sheet_url, len(items)

# ==== ìˆ˜ì§‘ ====
def fetch_entries_combined_or():
    """'1ì¼ OR 2ì¼ OR ... 31ì¼' í•œ ë²ˆë§Œ RSS í˜¸ì¶œ"""
    url = f"{BASE}?q={quote(OR_QUERY)}&{COMMON_QS}"
    feed = feedparser.parse(url)
    return feed.entries or []

def parse_published(entry):
    if hasattr(entry, "published_parsed") and entry.published_parsed:
        return datetime.fromtimestamp(time.mktime(entry.published_parsed), tz=timezone.utc)
    return datetime.now(timezone.utc)

def is_within_yesterday_or_today(pub_dt_utc: datetime) -> bool:
    kst = pub_dt_utc.astimezone(KST)
    return kst.date() in {YESTERDAY_KST, TODAY_KST}

def dedupe(items):
    """ë§í¬ ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ìˆ˜ì§‘ ë‹¨ê³„)"""
    seen = set()
    out = []
    for it in items:
        key = it["link"]
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def title_similarity(a: str, b: str) -> float:
    return SequenceMatcher(None, a.strip(), b.strip()).ratio()

def dedupe_for_sheet(items, title_similarity_threshold=0.70):
    """ì‹œíŠ¸ ì ì¬ ì „: URL ë™ì¼/ì œëª© ìœ ì‚¬(â‰¥threshold) ì œê±°"""
    seen_urls = set()
    kept = []
    kept_titles = []
    for it in items:
        url = it["link"]
        title = it["title"]
        if url in seen_urls:
            continue
        if kept_titles:
            sim = max(title_similarity(title, t) for t in kept_titles)
            if sim >= title_similarity_threshold:
                continue
        kept.append(it)
        kept_titles.append(title)
        seen_urls.add(url)
    return kept

# ==== ë³¸ë¬¸ ì¶”ì¶œ (ë³‘ë ¬) ====
def extract_article_text(url: str) -> str:
    try:
        resp = requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, allow_redirects=True)
        resp.raise_for_status()
        doc = Document(resp.text or "")
        summary_html = doc.summary()
        soup = BeautifulSoup(summary_html, "lxml")
        text = soup.get_text(separator="\n", strip=True)
        text = re.sub(r"\s+\n", "\n", text)
        text = re.sub(r"\n{2,}", "\n\n", text)
        return text[:15000]
    except Exception:
        return ""

def extract_bodies_parallel(urls):
    """URL ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ íŒŒì‹± â†’ {url: body}"""
    out = {}
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
        futures = {ex.submit(extract_article_text, u): u for u in urls}
        for fut in as_completed(futures):
            u = futures[fut]
            try:
                out[u] = fut.result()
            except Exception:
                out[u] = ""
    return out

# ==== â€œì£¼ê°€ ì˜í–¥â€ ìŠ¤ì½”ì–´ ====
MAJOR_COMPANY_EVENTS = [
    r"ì‹¤ì |ì˜ì—…ì´ìµ|ìˆœì´ìµ|ê°€ì´ë˜ìŠ¤|ëª©í‘œê°€|ìƒí–¥|í•˜í–¥|ì»¨ì„¼ì„œìŠ¤",
    r"ì¸ìˆ˜|í•©ë³‘|M&A|ë§¤ê°|ì§€ë¶„ ì·¨ë“|ì§€ë¶„ ë§¤ê°|ì „ëµì  ì œíœ´|JV",
    r"ëŒ€ê·œëª¨\s*ìˆ˜ì£¼|ìˆ˜ì£¼ ê³µì‹œ|ê³µê¸‰ ê³„ì•½|ì¥ê¸° ê³„ì•½|ë‚©í’ˆ",
    r"ë¦¬ì½œ|í’ˆì§ˆ ë¬¸ì œ|ì‚¬ê³ |í™”ì¬|ê³µì¥|ë¼ì¸ ì¤‘ë‹¨|íŒŒì—…",
    r"CEO|ëŒ€í‘œì´ì‚¬|ì‚¬ì„|í•´ì„|ì„ ì„|íš¡ë ¹|ë°°ì„|ìˆ˜ì‚¬|ì••ìˆ˜ìˆ˜ìƒ‰",
    r"ì¦ì|ìœ ìƒì¦ì|ê°ì|CB|BW|ì „í™˜ì‚¬ì±„|ë°°ë‹¹|ìì‚¬ì£¼|ì‹ ê·œ ìƒì¥|ìƒì¥íì§€|ê´€ë¦¬ì¢…ëª©",
    r"FDA|í’ˆëª©í—ˆê°€|í—ˆê°€ ì·¨ì†Œ|ì„ìƒ\s*(ì„±ê³µ|ì‹¤íŒ¨)|ê¸´ê¸‰ì‚¬ìš©ìŠ¹ì¸|ì‹ì•½ì²˜|EMA",
    r"ê³µì •ìœ„|ê³¼ì§•ê¸ˆ|ì œì¬|ë‹´í•©|ì¡°ì‚¬ ì°©ìˆ˜|ê²€ì°°",
]
INDUSTRY_WIDE = [
    r"ì—…í™©|ì‚¬ì´í´|ìˆ˜ìš” ë‘”í™”|ìˆ˜ìš” íšŒë³µ|ê°€ê²© ì¸ìƒ|ê°€ê²© ì¸í•˜|ê°ì‚°|ì¦ì‚°",
    r"ë©”ëª¨ë¦¬|DRAM|NAND|ë°˜ë„ì²´ ì¥ë¹„|ë¦¬íŠ¬|ë‹ˆì¼ˆ|ì½”ë°œíŠ¸|ì›ìì¬",
    r"ë³´ì¡°ê¸ˆ|ê·œì œ|ì™„í™”|ì˜ë¬´í™”|ì¹œí™˜ê²½|RE100|íƒ„ì†Œ|ìˆ˜ì¶œì… ê·œì œ",
]
GLOBAL_MACRO = [
    r"ì—°ì¤€|Fed|ê¸ˆë¦¬\s*(ì¸ìƒ|ì¸í•˜|ë™ê²°)|FOMC|ECB|BOJ|ì¤‘êµ­\s*ë¶€ì–‘|í™˜ìœ¨|ë‹¬ëŸ¬|ì—”í™”|ìœ„ì•ˆ",
    r"ìœ ê°€|WTI|ë¸Œë ŒíŠ¸|OPEC|ê°ì‚°",
    r"ì „ìŸ|ë¬´ë ¥|ë¶„ìŸ|ìš°í¬ë¼ì´ë‚˜|ì¤‘ë™|ëŒ€ë§Œ|ì œì¬|ìˆ˜ì¶œí†µì œ|ê´€ì„¸",
]
# ì§€ë„ì/ì •ì¹˜: í™˜ê²½ë³€ìˆ˜ë¡œ ë™ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥ (ê¸°ë³¸ì— íŠ¸ëŸ¼í”„/ì´ì¬ëª… í¬í•¨)
LEADER_NAMES = os.getenv("LEADER_NAMES", "íŠ¸ëŸ¼í”„|Trump|ì´ì¬ëª…|ëŒ€í†µë ¹|ë°±ì•…ê´€|ì²­ì™€ëŒ€")
POLITICAL = [
    rf"{LEADER_NAMES}|ì •ìƒíšŒë‹´|í–‰ì •ëª…ë ¹|ëŒ€ì±…|íŠ¹ë³„ë²•|ì¶”ê²½|ì˜ˆì‚°|ì •ì±… ë°œí‘œ|êµ­ë¬´íšŒì˜|êµ­íšŒ",
]

WEIGHTS = {
    "MAJOR_COMPANY_EVENTS": 4,
    "INDUSTRY_WIDE": 2,
    "GLOBAL_MACRO": 3,
    "POLITICAL": 3,
    "DATE_CONTEXT": 1,
    "TIME_CONTEXT": 1,
}
DATE_CONTEXT = r"ë¶€í„°|ê¹Œì§€|ì|ì‹œí–‰|ë§ˆê°|ê³µê³ |ë°œí‘œ|ê°œìµœ|ì ‘ìˆ˜|ì‹œí•œ|íš¨ë ¥|íš¨ê³¼"
TIME_CONTEXT = r"ì˜¤ì „|ì˜¤í›„|\d{1,2}\s*ì‹œ|\d{1,2}\s*ë¶„"

def _score_with_patterns(text: str, patterns, weight: int):
    score = 0
    for pat in patterns:
        if re.search(pat, text, re.IGNORECASE):
            score += weight
    return score

def market_moving_score(title: str, body: str) -> int:
    full = (title or "") + "\n" + (body or "")
    score = 0
    score += _score_with_patterns(full, MAJOR_COMPANY_EVENTS, WEIGHTS["MAJOR_COMPANY_EVENTS"])
    score += _score_with_patterns(full, INDUSTRY_WIDE, WEIGHTS["INDUSTRY_WIDE"])
    score += _score_with_patterns(full, GLOBAL_MACRO, WEIGHTS["GLOBAL_MACRO"])
    score += _score_with_patterns(full, POLITICAL, WEIGHTS["POLITICAL"])
    if re.search(DATE_CONTEXT, full): score += WEIGHTS["DATE_CONTEXT"]
    if re.search(TIME_CONTEXT, full): score += WEIGHTS["TIME_CONTEXT"]
    return score

def filter_market_moving(items, body_cache):
    """
    ì‹œì¥ì˜í–¥ ê¸°ì‚¬ë§Œ ì„ ë³„: ì œëª©ì—ì„œ ê°•ì‹ í˜¸ ì—†ìœ¼ë©´ ë³¸ë¬¸(ìºì‹œ or ë³‘ë ¬ ê²°ê³¼)ë¡œ ë³´ê°•.
    ì„ê³„ì¹˜ 4 ì´ìƒë§Œ ì±„íƒ.
    """
    high_sig_regex = "|".join([*MAJOR_COMPANY_EVENTS, *GLOBAL_MACRO, *POLITICAL])
    kept = []
    for it in items:
        title = it["title"]
        link  = it["link"]
        needs_body = not re.search(high_sig_regex, title, re.IGNORECASE)
        body = body_cache.get(link, "") if needs_body else ""
        score = market_moving_score(title, body)
        if score >= 4:
            it["mm_score"] = score
            kept.append(it)
    return kept

# ==== í…”ë ˆê·¸ë¨ ====
def send_tg(text: str):
    if not BOT_TOKEN or not CHAT_ID:
        raise RuntimeError("BOT_TOKEN/CHAT_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendMessage"
    r = requests.post(url, data={
        "chat_id": CHAT_ID,
        "text": text,
        "parse_mode": "HTML",
        "disable_web_page_preview": True
    }, timeout=30)
    r.raise_for_status()

def build_top5_message(items, sheet_url: str, sheet_count: int):
    total = len(items)
    top = items[:5]
    header = f"ğŸ“ˆ ì‹œì¥ì˜í–¥ ê°€ëŠ¥ì„± ë†’ì€ ê¸°ì‚¬ (ì–´ì œ/ì˜¤ëŠ˜)\nì„ ë³„ {total}ê±´ ì¤‘ TOP 5 ì•„ë˜ â¬‡ï¸\n"

    if not top:
        text = f"{header}\n(í•´ë‹¹ ê¸°ì‚¬ ì—†ìŒ)\n\nğŸ“Š ì „ì²´ ëª©ë¡({sheet_count}ê±´): {html.escape(sheet_url)}"
        return [text]

    lines = []
    for it in top:
        title = it['title'].strip()
        if len(title) > 150:
            title = title[:147] + "â€¦"
        link = it["link"]
        ts = it["published_kst"].strftime("%Y-%m-%d %H:%M")
        score = it.get("mm_score", 0)
        lines.append(f"â€¢ {html.escape(title)}\n{html.escape(link)}  <i>({ts} KST Â· score {score})</i>")

    body = "\n\n".join(lines)
    footer = f"\n\nğŸ“Š ì „ì²´ ëª©ë¡({sheet_count}ê±´): {html.escape(sheet_url)}"
    text = header + "\n" + body + footer

    if len(text) <= 4096:
        return [text]

    chunks, cur, size = [], [header], len(header)
    for block in lines + [footer]:
        block = "\n\n" + block
        if size + len(block) > 4096:
            chunks.append("".join(cur))
            cur, size = [block], len(block)
        else:
            cur.append(block); size += len(block)
    if cur: chunks.append("".join(cur))
    return chunks

# ==== ë©”ì¸ ====
def main():
    # 1) OR ê²€ìƒ‰ìœ¼ë¡œ í•œ ë²ˆë§Œ RSS í˜¸ì¶œ
    entries = fetch_entries_combined_or()

    # 2) í›„ë³´ êµ¬ì„± (ì–´ì œ/ì˜¤ëŠ˜ë§Œ)
    candidates = []
    for e in entries:
        title = e.get("title", "").strip()
        link  = e.get("link", "").strip()
        if not title or not link:
            continue
        pub_utc = parse_published(e)
        if not is_within_yesterday_or_today(pub_utc):
            continue
        candidates.append({
            "title": title,
            "link": link,
            "published_kst": pub_utc.astimezone(KST),
        })

    # 3) ë§í¬ ì¤‘ë³µ ì œê±°
    candidates = dedupe(candidates)

    # 4) ë‚ ì§œ íŒ¨í„´ í•„í„°: ì œëª© í†µê³¼ + (ì œëª© ë¯¸í†µê³¼ëŠ” ë³¸ë¬¸ ë³‘ë ¬ ì¶”ì¶œë¡œ 2ì°¨ í•„í„°)
    title_pass = [it for it in candidates if DATE_TERM_RE.search(it["title"])]
    need_body  = [it for it in candidates if not DATE_TERM_RE.search(it["title"])]

    bodies_for_date = {}
    if need_body:
        bodies_for_date = extract_bodies_parallel([it["link"] for it in need_body])

    date_filtered = []
    date_filtered.extend(title_pass)
    for it in need_body:
        body = bodies_for_date.get(it["link"], "")
        if body and DATE_TERM_RE.search(body):
            date_filtered.append(it)

    # 5) ìµœì‹ ìˆœ ì •ë ¬
    date_filtered.sort(key=lambda x: x["published_kst"], reverse=True)

    # 6) ì‹œì¥ì˜í–¥ ê¸°ì‚¬ ì„ ë³„: ì œëª© ê°•ì‹ í˜¸ ì—†ëŠ” ê²ƒë§Œ ì¶”ê°€ë¡œ **ë³‘ë ¬** ë³¸ë¬¸ ì¶”ì¶œ
    high_sig_regex = "|".join([*MAJOR_COMPANY_EVENTS, *GLOBAL_MACRO, *POLITICAL])
    to_fetch = [it["link"] for it in date_filtered if not re.search(high_sig_regex, it["title"], re.IGNORECASE)]
    body_cache = extract_bodies_parallel(to_fetch) if to_fetch else {}

    market_items = filter_market_moving(date_filtered, body_cache)
    market_items.sort(key=lambda x: (x.get("mm_score", 0), x["published_kst"]), reverse=True)

    # 7) ì „ì²´(ì¤‘ë³µ ì–µì œ ë²„ì „) â†’ ì‹œíŠ¸ ê¸°ë¡
    sh = ensure_gspread()
    sheet_url, sheet_count = write_sheet_all(sh, date_filtered)

    # 8) í…”ë ˆê·¸ë¨: ì‹œì¥ì˜í–¥ TOP 5ë§Œ
    texts = build_top5_message(market_items, sheet_url, sheet_count)
    for t in texts:
        send_tg(t)

if __name__ == "__main__":
    main()
